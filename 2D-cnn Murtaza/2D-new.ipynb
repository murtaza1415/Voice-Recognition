{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy import signal\n",
    "from sklearn.svm import SVC\n",
    "from IPython import display\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "#from keras.utils import model_to_dot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, Dropout, MaxPooling2D, AveragePooling2D, Flatten, Dense, Activation, BatchNormalization\n",
    "from python_speech_features import mfcc\n",
    "\n",
    "#For fft spectrum.\n",
    "import sigproc\n",
    "import constants as c\n",
    "from scipy.signal import lfilter, butter\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "#import psutil\n",
    "#p = psutil.Process()\n",
    "#p.cpu_affinity([0,1,2,5,9,13,17,18,19,20,23,27,28,29,30,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "no_batches = 0\n",
    "batch_sum = 0\n",
    "curr_size = batch_size\n",
    "\n",
    "while (batch_sum + curr_size) < 1000000:\n",
    "    batch_sum += curr_size\n",
    "    no_batches += 1\n",
    "    if no_batches%10 == 0:\n",
    "        curr_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'dim': (512,299),\n",
    "                'batch_size': 256,\n",
    "                'n_classes': 5994,\n",
    "                'n_channels': 1,\n",
    "                'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_folders():\n",
    "    '''\n",
    "    Rename all speaker directories in numeric order to represent the speaker's ID.\n",
    "    '''    \n",
    "    speakers = glob(\"/data/techresearch/Murtaza/vox2/dev/aac/*\")    \n",
    "    #Make sure only valid speaker directories are in list.\n",
    "    for speaker in speakers:    \n",
    "        assert os.path.isdir(speaker)\n",
    "        assert 'id' in speaker\n",
    "    for i in range(0,len(speakers)):\n",
    "        old_path = speakers[i] \n",
    "        new_path = os.path.split(speakers[i])[0] + '/' + str(i) \n",
    "        os.rename(old_path, new_path)\n",
    "    print('Folders renamed: ' + str(len(speakers)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_audios():\n",
    "    '''\n",
    "    Rearrange all the files in the dataset so that all audios\n",
    "    of a particular speaker are in the same directory.\n",
    "    '''\n",
    "    speakers = glob(\"/data/techresearch/Murtaza/vox2/dev/aac/*/\") \n",
    "    counter = 0\n",
    "    for speaker in speakers:\n",
    "        subdirs = os.listdir(speaker)                         #List subdirectories for a particular speaker.\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        for j in range(len(subdirs)):\n",
    "            subdir = subdirs[j]\n",
    "            if os.path.isdir(speaker + subdir):               #Check if its a directory and not a file.\n",
    "                audio_files = os.listdir(speaker + subdir)    #List files for a particular speaker in a particular subfolder.\n",
    "                for audio_file in audio_files:\n",
    "                    if audio_file.endswith('.m4a'):          #Check if file is audio file and not a text file or something unwanted.\n",
    "                        old_path = speaker + subdir + '/' + audio_file \n",
    "                        new_path = speaker + str(j) + '_' + audio_file\n",
    "                        os.rename(old_path, new_path)         #Move the audiofile to main speaker directory.\n",
    "                shutil.rmtree(speaker + subdir)               #Remove subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_audio_files():\n",
    "    '''\n",
    "    Rename all audios to include speaker ID in their name.\n",
    "    '''\n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/dev/aac/*/*.m4a\")\n",
    "    for path in audio_files:\n",
    "        label = os.path.split(os.path.split(path)[0])[1]\n",
    "        path_pre = os.path.split(path)[0]\n",
    "        path_post = os.path.split(path)[1]\n",
    "        new_path = path_pre + '/' + label + '_' + path_post\n",
    "        os.rename(path, new_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_split():\n",
    "    '''\n",
    "    Set aside a small portion of the data for validation.\n",
    "    '''\n",
    "    val_size = 8000\n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/dev/aac/*/*.m4a\")\n",
    "    random.shuffle(audio_files)\n",
    "    val_files = audio_files[:8000]\n",
    "    for path in val_files:\n",
    "        pre_path = '/data/techresearch/Murtaza/vox2/val/aac/'\n",
    "        post_path = os.path.split(path)[1]\n",
    "        new_path = '/data/techresearch/Murtaza/vox2/val/aac/' + post_path\n",
    "        os.rename(path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for fft spectrum. Copied from 'https://github.com/linhdvu14/vggvox-speaker-identification/blob/master'\n",
    "def remove_dc_and_dither(sin, sample_rate):\n",
    "\tif sample_rate == 16e3:\n",
    "\t\talpha = 0.99\n",
    "\telif sample_rate == 8e3:\n",
    "\t\talpha = 0.999\n",
    "\telse:\n",
    "\t\tprint(\"Sample rate must be 16kHz or 8kHz only\")\n",
    "\t\texit(1)\n",
    "\tsin = lfilter([1,-1], [1,-alpha], sin)\n",
    "\tdither = np.random.random_sample(len(sin)) + np.random.random_sample(len(sin)) - 1\n",
    "\tspow = np.std(dither)\n",
    "\tsout = sin + 1e-6 * spow * dither\n",
    "\treturn sout\n",
    "        \n",
    "\n",
    "def normalize_frames(m,epsilon=1e-12):\n",
    "\treturn np.array([(v - np.mean(v)) / max(np.std(v),epsilon) for v in m])\n",
    "\n",
    "\n",
    "def get_fft_spectrum(signal, buckets=None):\n",
    "\t#signal = load_wav(filename,c.SAMPLE_RATE)\n",
    "\tsignal *= 2**15\n",
    "\n",
    "\t# get FFT spectrum\n",
    "\tsignal = remove_dc_and_dither(signal, c.SAMPLE_RATE)\n",
    "\t#signal = sigproc.preemphasis(signal, coeff=c.PREEMPHASIS_ALPHA)\n",
    "\tframes = sigproc.framesig(signal, frame_len=c.FRAME_LEN*c.SAMPLE_RATE, frame_step=c.FRAME_STEP*c.SAMPLE_RATE, winfunc=np.hamming)\n",
    "\tfft = abs(np.fft.fft(frames,n=c.NUM_FFT))\n",
    "\tfft_norm = normalize_frames(fft.T)\n",
    "\n",
    "\t# truncate to max bucket sizes\n",
    "\t#rsize = max(k for k in buckets if k <= fft_norm.shape[1])\n",
    "\t#rstart = int((fft_norm.shape[1]-rsize)/2)\n",
    "\t#out = fft_norm[:,rstart:rstart+rsize]\n",
    "\n",
    "\t#return out\n",
    "\treturn fft_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note: Study required for optimal spectogram.'''\n",
    "\n",
    "def audio_to_image(audio, sr):                            \n",
    "    '''\n",
    "    Convert an audio to its spectrogram.\n",
    "    '''\n",
    "    #_,_,spectrogram = signal.spectrogram(audio, sr)\n",
    "    #audio_mfcc = librosa.feature.mfcc(y=audio, sr=16000,  n_mfcc=60)\n",
    "    #audio_mfcc2 = mfcc(signal=audio, samplerate=16000, nfft=512, winlen=0.025, winstep=0.009, nfilt=400, numcep = 300, winfunc=np.hamming)\n",
    "    audio_mfcc3 = get_fft_spectrum(audio)\n",
    "    return audio_mfcc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(path):\n",
    "    '''\n",
    "    Extract label of audio from the given path.\n",
    "    '''\n",
    "    label = os.path.split(os.path.split(path)[0])[1]\n",
    "    label = int(label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_enroll(path):\n",
    "    '''\n",
    "    Extract label of audio from \n",
    "    the given path.\n",
    "    '''\n",
    "    label = path.split('/')[-1]\n",
    "    label = label.replace('.wav', '')\n",
    "    label = int(label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_eval(path):\n",
    "    '''\n",
    "    Extract label of audio from \n",
    "    the given path.\n",
    "    '''\n",
    "    label = path.split('_')[-2]\n",
    "    label = int(label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_val_vox2(path):\n",
    "    '''\n",
    "    Extract label of audio from \n",
    "    the given path.\n",
    "    '''\n",
    "    label = path.split('/')[-1]\n",
    "    label = label.split('_')[-3]\n",
    "    label = int(label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def load_training_dataset_old(root_path, no_speakers):\n",
    "    '''\n",
    "    #Returns the complete dataset for training and the associated labels.\n",
    "    #Note that the audios are converted to images.\n",
    "    '''\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    speakers = glob(root_path + '/*/')    #List of speaker directories.\n",
    "    speakers = speakers[:no_speakers]\n",
    "    \n",
    "    for j,speaker in enumerate(speakers):\n",
    "        audio_files = glob(speaker + '*.wav')\n",
    "        \n",
    "        for audio_file in audio_files:            \n",
    "            label = get_label(audio_file)\n",
    "            audio_data, _ = librosa.load(sr=audio_rate, mono=True, path=audio_file)\n",
    "            \n",
    "            assert _ == audio_rate\n",
    "            no_clips = len(audio_data) // (audio_rate*no_seconds)\n",
    "            \n",
    "            for i in range(0, no_clips):                                                                #4 sec clips\n",
    "                audio_clip = audio_data[(audio_rate*no_seconds)*i:(audio_rate*no_seconds)*(i+1)]\n",
    "                image = audio_to_image(audio_clip, audio_rate)\n",
    "                x_train.append(image)\n",
    "                y_train.append(label)\n",
    "        \n",
    "        print('No. speakers loaded: ' + str(j + 1))    \n",
    "    return x_train, y_train\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset_vox1():\n",
    "    '''\n",
    "    Create the complete dataset for training and the associated labels. \n",
    "    The dataset is created and saved in subsets due to limitaion of RAM.\n",
    "    Note that the audios are converted to images.\n",
    "    '''\n",
    "    \n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox1/dev/wav/*/*.wav\")\n",
    "    random.shuffle(audio_files)\n",
    "    \n",
    "    subset_size = 9000\n",
    "    total_subsets = 16\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    count = 0\n",
    "    for j in range(total_subsets):\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        audio_subset = audio_files[ (subset_size*j) : (subset_size*(j+1)) ]\n",
    "        print(str((subset_size*j)) + ' to ' + str(subset_size*(j+1)))\n",
    "\n",
    "        for audio_file in audio_subset:\n",
    "            count+=1\n",
    "            print(count)\n",
    "            label = get_label(audio_file)\n",
    "            audio_data, _ = librosa.load(sr=audio_rate, mono=True, path=audio_file)\n",
    "\n",
    "            assert _ == audio_rate\n",
    "            no_clips = len(audio_data) // (audio_rate*no_seconds)\n",
    "\n",
    "            for i in range(0, no_clips):                                                                #4 sec clips\n",
    "                audio_clip = audio_data[(audio_rate*no_seconds)*i:(audio_rate*no_seconds)*(i+1)]\n",
    "                image = audio_to_image(audio_clip, audio_rate)\n",
    "                x_train.append(image)\n",
    "                y_train.append(label)\n",
    "    \n",
    "        #Convert to numpy\n",
    "        x_train = np.asarray(x_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        #Save the numpy data\n",
    "        np.save('/data/techresearch/Murtaza/mfcc3/x_train_' + str(j) + '.npy', x_train)\n",
    "        np.save('/data/techresearch/Murtaza/mfcc3/y_train_' + str(j) + '.npy', y_train)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enrollment_dataset():\n",
    "    '''\n",
    "    Returns the complete dataset for enrollment and the associated labels.\n",
    "    Note that the audios are converted to images.\n",
    "    '''\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    audio_files = glob('/home/techresearch/rnn/app_enrollment/data/enrollment/*.wav')\n",
    "    random.shuffle(audio_files)\n",
    "    for j, audio_file in enumerate(audio_files):            \n",
    "        label = get_label_enroll(audio_file)\n",
    "        audio_data, _ = librosa.load(sr=audio_rate, mono=True, path=audio_file)\n",
    "\n",
    "        assert _ == audio_rate\n",
    "        no_clips = len(audio_data) // (audio_rate*no_seconds)\n",
    "\n",
    "        for i in range(0, no_clips):                                                                #4 sec clips\n",
    "            audio_clip = audio_data[(audio_rate*no_seconds)*i:(audio_rate*no_seconds)*(i+1)]\n",
    "            image = audio_to_image(audio_clip, audio_rate)\n",
    "            x_train.append(image)\n",
    "            y_train.append(label)\n",
    "        print('No. Speakers loaded: ' + str(j + 1))\n",
    "    \n",
    "    #Convert to numpy\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset():\n",
    "    '''\n",
    "    Returns the complete dataset for evaluation and the associated labels.\n",
    "    Note that the audios are converted to images.\n",
    "    '''\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    audio_files = glob('/home/techresearch/rnn/app_enrollment/data/evaluation_original/*.wav')\n",
    "    random.shuffle(audio_files)\n",
    "    for audio_file in audio_files:            \n",
    "        label = get_label_eval(audio_file)\n",
    "        audio_data, _ = librosa.load(sr=audio_rate, mono=True, path=audio_file)\n",
    "\n",
    "        assert _ == audio_rate\n",
    "        no_clips = len(audio_data) // (audio_rate*no_seconds)\n",
    "\n",
    "        for i in range(0, 1):                                                                #3 sec clips\n",
    "            audio_clip = audio_data[(audio_rate*no_seconds)*i:(audio_rate*no_seconds)*(i+1)]\n",
    "            image = audio_to_image(audio_clip, audio_rate)\n",
    "            x_train.append(image)\n",
    "            y_train.append(label)\n",
    "    \n",
    "    #Convert to numpy\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob('/home/techresearch/rnn/app_enrollment/data/evaluation_original/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val_dataset_vox2():\n",
    "    '''\n",
    "    Create the complete dataset for validation and the associated labels.\n",
    "    Note that the audios are converted to images.\n",
    "    '''\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    count = 0\n",
    "    \n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/val/wav/*.wav\")\n",
    "    random.shuffle(audio_files)\n",
    "    for j, audio_file in enumerate(audio_files):\n",
    "        label = get_label_val_vox2(audio_file)\n",
    "        audio_data, _ = librosa.load(sr=None, mono=True, path=audio_file)\n",
    "\n",
    "        assert _ == audio_rate\n",
    "        no_clips = len(audio_data) // (audio_rate*no_seconds)\n",
    "\n",
    "        if len(audio_data) >= (audio_rate*no_seconds):          \n",
    "            audio_clip = audio_data[0 : audio_rate*no_seconds]\n",
    "            image = audio_to_image(audio_clip, audio_rate)\n",
    "            x_train.append(image)\n",
    "            y_train.append(label)\n",
    "            \n",
    "        count +=1\n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    #Convert to numpy\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    #Save the numpy data\n",
    "    np.save('/data/techresearch/Murtaza/vox2/val/x_val' + '.npy', x_train)\n",
    "    np.save('/data/techresearch/Murtaza/vox2/val/y_val' + '.npy', y_train)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def vox2_iterative_generative_training(model):\n",
    "\n",
    "    \n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/dev/aac/*/*.m4a\")\n",
    "    random.shuffle(audio_files)\n",
    "    \n",
    "    no_epochs = 15\n",
    "    subset_size = 5000\n",
    "    total_subsets = len(audio_files)//subset_size\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    count = 0\n",
    "    for epoch in range(0,no_epochs):\n",
    "        \n",
    "        for j in range(0,total_subsets):\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "            count = 0\n",
    "            \n",
    "            audio_subset = audio_files[ (subset_size*j) : (subset_size*(j+1)) ]\n",
    "            print(str((subset_size*j)) + ' to ' + str(subset_size*(j+1)))\n",
    "\n",
    "            for audio_file in audio_subset:\n",
    "                count +=1\n",
    "                print(count)\n",
    "                label = get_label(audio_file)\n",
    "                audio_data, _ = librosa.load(sr=audio_rate, mono=True, path=audio_file)\n",
    "        \n",
    "                assert _ == audio_rate\n",
    "\n",
    "                if len(audio_data)/audio_rate >= no_seconds:\n",
    "                    start = random.randint(0, len(audio_data) - (audio_rate*no_seconds) )\n",
    "                    end = start + (audio_rate*no_seconds)\n",
    "                    audio_clip = audio_data[start : end]\n",
    "                    image = audio_to_image(audio_clip, audio_rate)\n",
    "                    x_train.append(image)\n",
    "                    y_train.append(label)\n",
    "                    count += 1\n",
    "                    print(count)\n",
    "\n",
    "            #Convert to numpy\n",
    "            x_train = np.asarray(x_train)\n",
    "            y_train = np.asarray(y_train)\n",
    "            \n",
    "            #Reshape x_train to have 1 channel.\n",
    "            x_train = np.reshape(x_train, [-1,512,299,1])\n",
    "            #Onehot encoding for y_train.\n",
    "            y_train = to_categorical(y_train, num_classes=5993)\n",
    "            \n",
    "            print('Epoch: ' + str(epoch))\n",
    "            model.fit(x=x_train, y=y_train, batch_size=32, epochs=1, validation_split=0.03)\n",
    "             \n",
    "    \n",
    "    return\n",
    "'''\n",
    "\n",
    "def vox2_iterative_generative_training(model, feature_model1, feature_model2):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/dev/wav/*/*.wav\")\n",
    "    random.shuffle(audio_files)\n",
    "    \n",
    "    no_epochs = 15\n",
    "    subset_size = 5000\n",
    "    total_subsets = len(audio_files)//subset_size\n",
    "    audio_rate = 16000\n",
    "    no_seconds = 3\n",
    "    \n",
    "    for epoch in range(0,no_epochs):\n",
    "        \n",
    "        for j in range(0,total_subsets):\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            audio_subset = audio_files[ (subset_size*j) : (subset_size*(j+1)) ]\n",
    "            print(str((subset_size*j)) + ' to ' + str(subset_size*(j+1)))\n",
    "\n",
    "            for audio_file in audio_subset:\n",
    "                label = get_label(audio_file)\n",
    "                #audio_data, _ = librosa.load(sr=audio_rate, mono=True, path=audio_file)\n",
    "                audio_data, _ = librosa.load(sr=None, mono=True, path=audio_file)\n",
    "                #audio_data = AudioSegment.from_file(audio_file)\n",
    "                #audio_data = audio_data.get_array_of_samples()\n",
    "                #audio_data = np.array(audio_data)\n",
    "\n",
    "                #assert _ == audio_rate\n",
    "\n",
    "                if len(audio_data)/audio_rate >= no_seconds:\n",
    "                    start = random.randint(0, len(audio_data) - (audio_rate*no_seconds) )\n",
    "                    end = start + (audio_rate*no_seconds)\n",
    "                    audio_clip = audio_data[start : end]\n",
    "                    image = audio_to_image(audio_clip, audio_rate)\n",
    "                    x_train.append(image)\n",
    "                    y_train.append(label)\n",
    "\n",
    "            #Convert to numpy\n",
    "            x_train = np.asarray(x_train)\n",
    "            y_train = np.asarray(y_train)\n",
    "            \n",
    "            #Reshape x_train to have 1 channel.\n",
    "            x_train = np.reshape(x_train, [-1,512,299,1])\n",
    "            #Onehot encoding for y_train.\n",
    "            y_train = to_categorical(y_train, num_classes=5994)\n",
    "            \n",
    "            print('Epoch: ' + str(epoch))\n",
    "            model.fit(x=x_train, y=y_train, batch_size=32, epochs=1, validation_split=0.03)\n",
    "            \n",
    "            if (j+1)%40 == 0:\n",
    "                \n",
    "                x_train = np.load('/data/techresearch/Murtaza/vox2/val/x_val.npy')\n",
    "                y_train = np.load('/data/techresearch/Murtaza/vox2/val/y_val.npy')\n",
    "                #Reshape x_train to have 1 channel.\n",
    "                x_train = np.reshape(x_train, [-1,512,299,1])\n",
    "                #Onehot encoding for y_train.\n",
    "                y_train = to_categorical(y_train)\n",
    "                eval_result = model.evaluate(x=x_train, y=y_train) \n",
    "                \n",
    "                file = open('train_log.txt', 'a')\n",
    "                file.write('Epoch: ' + str(epoch) + '\\n')\n",
    "                file.write(str((subset_size*j)) + ' to ' + str(subset_size*(j+1)) + '\\n')\n",
    "                file.write(str(eval_result))\n",
    "                file.write('\\n\\n')\n",
    "                file.close\n",
    "                \n",
    "                model.save('/data/techresearch/Murtaza/vox2/dev/weights/model_' + str(epoch+1) + '_' + str(j+1) + '.h5')\n",
    "                feature_model1.save('/data/techresearch/Murtaza/vox2/dev/weights/featuremodel1_' + str(epoch+1) + '_' + str(j+1) + '.h5')\n",
    "                feature_model2.save('/data/techresearch/Murtaza/vox2/dev/weights/featuremodel2_' + str(epoch+1) + '_' + str(j+1) + '.h5')\n",
    "                model.save_weights('/data/techresearch/Murtaza/vox2/dev/weights/weights_' + str(epoch+1) + '_' + str(j+1) + '.h5')             \n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "subset_size = 5000\n",
    "j = 5\n",
    "eval_result = [0.0045, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log():\n",
    "    file = open('train_log.txt', 'a')\n",
    "    file.write('Epoch: ' + str(epoch) + '\\n')\n",
    "    file.write(str((subset_size*j)) + ' to ' + str(subset_size*(j+1)) + '\\n')\n",
    "    file.write(str(eval_result))\n",
    "    file.write('\\n\\n')\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('/data/techresearch/Murtaza/vox2/val/x_val.npy')\n",
    "y_train = np.load('/data/techresearch/Murtaza/vox2/val/y_val.npy')\n",
    "#Reshape x_train to have 1 channel.\n",
    "x_train = np.reshape(x_train, [-1,512,299,1])\n",
    "#Onehot encoding for y_train.\n",
    "y_train = to_categorical(y_train)\n",
    "eval_result = model.evaluate(x=x_train, y=y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob(\"/data/techresearch/Murtaza/vox2/val/wav/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "audio = AudioSegment.from_file(audio_file)\n",
    "audio = audio.get_array_of_samples()\n",
    "audio = np.array(audio)\n",
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sksound\n",
    "mySound = sksound.sounds.Sound(audio_file)\n",
    "mySound.read_sound(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySound.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(data=mySound.data, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for audio_file in audio_files:\n",
    "    audio_data, _ = librosa.load(sr=None, mono=True, path=audio_file)\n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vox2_val_to_wav():\n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/val/aac/*.m4a\")\n",
    "    export_dir = '/data/techresearch/Murtaza/vox2/val/wav/'\n",
    "    count = 0\n",
    "    for file in audio_files:\n",
    "        file_name = os.path.split(file)[1]\n",
    "        file_name = file_name.replace('.m4a','')\n",
    "        audio = AudioSegment.from_file(file)\n",
    "        audio.export(export_dir + file_name + '.wav', format='wav')\n",
    "        count += 1\n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vox2_dev_to_wav():\n",
    "    audio_files = glob(\"/data/techresearch/Murtaza/vox2/dev/aac/*/*.m4a\")\n",
    "    export_dir = '/data/techresearch/Murtaza/vox2/dev/wav/'\n",
    "    count = 0\n",
    "    for file in audio_files:\n",
    "        dir_name = os.path.split(os.path.split(file)[0])[1]\n",
    "        file_name = os.path.split(file)[1]\n",
    "        file_name = file_name.replace('.m4a','')\n",
    "        audio = AudioSegment.from_file(file)\n",
    "        audio.export(export_dir + dir_name + '/' + file_name + '.wav', format='wav')\n",
    "        count +=1\n",
    "        if (count%100) == 0:\n",
    "            print(count)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_correction():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(x,y):\n",
    "    combined = list(zip(x,y))\n",
    "    random.shuffle(combined)\n",
    "    x, y = zip(*combined)\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    '''\n",
    "    Creates and returns a new model.\n",
    "    '''\n",
    "    \n",
    "    #For Spectogram Input features (No convergence)\n",
    "    '''\n",
    "    model_input = Input(shape=(129,285,1))\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, kernel_size=(7, 7), strides=(2, 2), activation='relu')(model_input)\n",
    "    mpool1 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), activation='relu')(mpool1)\n",
    "    #mpool2 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv2)\n",
    "    conv4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv3)\n",
    "    conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv4)\n",
    "    \n",
    "    flatten = Flatten()(conv5)\n",
    "    \n",
    "    fc7 = Dense(1024)(flatten)\n",
    "    fc8 = Dense(1200, activation='softmax')(fc7)\n",
    "    \n",
    "    model = Model(model_input, fc8)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #For MFCC1 input features (Acc: 84, val_acc:70)\n",
    "    '''\n",
    "    model_input = Input(shape=(60,126,1))\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, kernel_size=(7, 7), strides=(2, 2), activation='relu')(model_input)\n",
    "    mpool1 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), activation='relu')(mpool1)\n",
    "    #mpool2 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv2)\n",
    "    #conv4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv3)\n",
    "    #conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv4)\n",
    "    \n",
    "    flatten = Flatten()(conv3)\n",
    "    \n",
    "    fc7 = Dense(1024)(flatten)\n",
    "    fc8 = Dense(1200, activation='softmax')(fc7)\n",
    "    \n",
    "    model = Model(model_input, fc8)\n",
    "    '''\n",
    "    \n",
    "    #For mfcc2 Input features (Good Convegence was witnessed)\n",
    "    '''\n",
    "    model_input = Input(shape=(332,300,1))\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, kernel_size=(7, 7), strides=(2, 2), activation='relu')(model_input)\n",
    "    mpool1 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), activation='relu')(mpool1)\n",
    "    mpool2 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu')(mpool2)\n",
    "    conv4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv3)\n",
    "    conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu')(conv4)\n",
    "    \n",
    "    flatten = Flatten()(conv5)\n",
    "    \n",
    "    fc7 = Dense(1024)(flatten)\n",
    "    fc8 = Dense(1200, activation='softmax')(fc7)\n",
    "    \n",
    "    model = Model(model_input, fc8)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #For mfcc3 Input features\n",
    "\n",
    "    model_input = Input(shape=(512,299,1))\n",
    "    #model_input_1 = BatchNormalization(scale=False, axis=3)(model_input)\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, kernel_size=(7, 7), strides=(2, 2))(model_input)\n",
    "    conv1 = BatchNormalization(scale=False, axis=3)(conv1)\n",
    "    conv1 = Activation('relu')(conv1) \n",
    "    mpool1 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2))(mpool1)\n",
    "    conv2 = BatchNormalization(scale=False, axis=3)(conv2)\n",
    "    conv2 = Activation('relu')(conv2) \n",
    "    mpool2 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1))(mpool2)\n",
    "    conv3 = BatchNormalization(scale=False, axis=3)(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1))(conv3)\n",
    "    conv4 = BatchNormalization(scale=False, axis=3)(conv4)\n",
    "    conv4 = Activation('relu')(conv4) \n",
    "    \n",
    "    conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1))(conv4)\n",
    "    conv5 = BatchNormalization(scale=False, axis=3)(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Dropout(0.15)(conv5)\n",
    "    mpool5 = MaxPooling2D(pool_size=(3, 3), strides=(2,2))(conv5)\n",
    "    \n",
    "    fc6 = Conv2D(filters=4096, kernel_size=(11, 1), strides=(1, 1))(mpool5)\n",
    "    fc6 = BatchNormalization(scale=False, axis=3)(fc6)\n",
    "    fc6 = Activation('relu')(fc6) \n",
    "    fc6 = Dropout(0.2)(fc6)\n",
    "    apool6 = AveragePooling2D(pool_size=(1, 5), strides=(1,1))(fc6)\n",
    "    \n",
    "    flatten = Flatten()(apool6)\n",
    "    \n",
    "    fc7 = Dense(1024, activation='relu')(flatten)\n",
    "    fc7 = Dropout(0.2)(fc7)\n",
    "    fc8 = Dense(5994, activation='softmax')(fc7)\n",
    "    \n",
    "    model = Model(model_input, fc8)\n",
    "    feature_model1 = Model(model_input, flatten)\n",
    "    feature_model2 = Model(model_input, fc7)\n",
    "    \n",
    "    \n",
    "    return model, feature_model1, feature_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, _ = librosa.load(sr=16000, mono=True, path='/home/techresearch/rnn/hello.wav')\n",
    "audio_data = audio_data[0:16000*3]\n",
    "audio_mfcc1 = librosa.feature.mfcc(y=audio_data, sr=16000,  n_mfcc=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, _ = librosa.load(sr=16000, mono=True, path='/home/techresearch/rnn/hello.wav')\n",
    "audio_data = audio_data[0:16000*3]\n",
    "audio_mfcc2 = mfcc(signal=audio_data, samplerate=16000, nfft=512, winlen=0.025, winstep=0.006, nfilt=400, numcep = 300, winfunc=np.hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, _ = librosa.load(sr=16000, mono=True, path='/home/techresearch/rnn/hello.wav')\n",
    "audio_data = audio_data[0:16000*3]\n",
    "audio_mfcc3 = get_fft_spectrum(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(audio_mfcc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mfcc4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(data = audio_data, rate = 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Load and shuffle dataset\n",
    "x_train, y_train = load_training_dataset('/data/techresearch/Murtaza/vox1/dev/wav/', 600)\n",
    "x_train, y_train = shuffle_dataset(x_train, y_train)\n",
    "#Convert to numpy\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "#Save the numpy data\n",
    "np.save('/data/techresearch/Murtaza/x_train.npy', x_train)\n",
    "np.save('/data/techresearch/Murtaza/y_train.npy', y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_training(model, feature_model, no_epochs):\n",
    "    for epoch in range(0,no_epochs):\n",
    "        for batch in range(0,16):\n",
    "            #Load the numpy Data from disk:\n",
    "            x_train = np.load('/data/techresearch/Murtaza/mfcc3_gpu/x_train_' + str(batch) + '.npy')\n",
    "            y_train = np.load('/data/techresearch/Murtaza/mfcc3_gpu/y_train_' + str(batch) + '.npy')\n",
    "            #Reshape x_train to have 1 channel.\n",
    "            x_train = np.reshape(x_train, [-1,512,299,1])\n",
    "            #Onehot encoding for y_train.\n",
    "            y_train = to_categorical(y_train)\n",
    "            \n",
    "            #Train the model.\n",
    "            print('Epoch: ' + str(epoch + 1))\n",
    "            print('Batch: ' + str(batch))\n",
    "            model.fit(x=x_train, y=y_train, batch_size=64, epochs=1, validation_split=0.03)\n",
    "            \n",
    "        model.save('/data/techresearch/Murtaza/mfcc3_gpu/weights/model_' + str(epoch+1) + '.h5')\n",
    "        feature_model.save('/data/techresearch/Murtaza_gpu/mfcc3/weights/feature_' + str(epoch+1) + '.h5')\n",
    "        model.save_weights('/data/techresearch/Murtaza_gpu/mfcc3/weights/weights_' + str(epoch+1) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prepare the model.\n",
    "model, feature_model1, feature_model2 = create_model()\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Learning Rate\n",
    "#K.set_value(model.optimizer.lr, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#iterative_training(model, feature_model, 3)\n",
    "vox2_iterative_generative_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/data/techresearch/Murtaza/mfcc3/weights/model_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('/data/techresearch/Murtaza/vox2/val/x_val.npy')\n",
    "y_train = np.load('/data/techresearch/Murtaza/vox2/val/y_val.npy')\n",
    "#Reshape x_train to have 1 channel.\n",
    "x_train = np.reshape(x_train, [-1,512,299,1])\n",
    "#Onehot encoding for y_train.\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.evaluate(x=x_train, y=y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enroll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_model = load_model('/data/techresearch/Murtaza/vox2/dev/weights4/model_4_35.h5')\n",
    "feature_model.outputs = [feature_model.layers[-2].output]\n",
    "feature_model.layers[-2].outbound_nodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Speakers loaded: 1\n",
      "No. Speakers loaded: 2\n",
      "No. Speakers loaded: 3\n",
      "No. Speakers loaded: 4\n",
      "No. Speakers loaded: 5\n",
      "No. Speakers loaded: 6\n",
      "No. Speakers loaded: 7\n",
      "No. Speakers loaded: 8\n",
      "No. Speakers loaded: 9\n",
      "No. Speakers loaded: 10\n",
      "No. Speakers loaded: 11\n",
      "No. Speakers loaded: 12\n",
      "No. Speakers loaded: 13\n",
      "No. Speakers loaded: 14\n",
      "No. Speakers loaded: 15\n",
      "No. Speakers loaded: 16\n",
      "No. Speakers loaded: 17\n",
      "No. Speakers loaded: 18\n",
      "No. Speakers loaded: 19\n",
      "No. Speakers loaded: 20\n",
      "No. Speakers loaded: 21\n",
      "No. Speakers loaded: 22\n",
      "No. Speakers loaded: 23\n",
      "No. Speakers loaded: 24\n",
      "No. Speakers loaded: 25\n",
      "No. Speakers loaded: 26\n",
      "No. Speakers loaded: 27\n",
      "No. Speakers loaded: 28\n",
      "No. Speakers loaded: 29\n",
      "No. Speakers loaded: 30\n",
      "No. Speakers loaded: 31\n",
      "No. Speakers loaded: 32\n",
      "No. Speakers loaded: 33\n",
      "No. Speakers loaded: 34\n",
      "No. Speakers loaded: 35\n",
      "No. Speakers loaded: 36\n",
      "No. Speakers loaded: 37\n",
      "No. Speakers loaded: 38\n",
      "No. Speakers loaded: 39\n",
      "No. Speakers loaded: 40\n",
      "No. Speakers loaded: 41\n",
      "No. Speakers loaded: 42\n",
      "No. Speakers loaded: 43\n",
      "No. Speakers loaded: 44\n",
      "No. Speakers loaded: 45\n",
      "No. Speakers loaded: 46\n",
      "No. Speakers loaded: 47\n",
      "No. Speakers loaded: 48\n",
      "No. Speakers loaded: 49\n",
      "No. Speakers loaded: 50\n",
      "No. Speakers loaded: 51\n",
      "No. Speakers loaded: 52\n",
      "No. Speakers loaded: 53\n",
      "No. Speakers loaded: 54\n",
      "No. Speakers loaded: 55\n",
      "No. Speakers loaded: 56\n",
      "No. Speakers loaded: 57\n",
      "No. Speakers loaded: 58\n",
      "No. Speakers loaded: 59\n",
      "No. Speakers loaded: 60\n",
      "No. Speakers loaded: 61\n",
      "No. Speakers loaded: 62\n",
      "No. Speakers loaded: 63\n",
      "No. Speakers loaded: 64\n",
      "No. Speakers loaded: 65\n",
      "No. Speakers loaded: 66\n",
      "No. Speakers loaded: 67\n",
      "No. Speakers loaded: 68\n",
      "No. Speakers loaded: 69\n",
      "No. Speakers loaded: 70\n",
      "No. Speakers loaded: 71\n",
      "No. Speakers loaded: 72\n",
      "No. Speakers loaded: 73\n",
      "No. Speakers loaded: 74\n",
      "No. Speakers loaded: 75\n",
      "No. Speakers loaded: 76\n",
      "No. Speakers loaded: 77\n",
      "No. Speakers loaded: 78\n",
      "No. Speakers loaded: 79\n",
      "No. Speakers loaded: 80\n",
      "No. Speakers loaded: 81\n",
      "No. Speakers loaded: 82\n",
      "No. Speakers loaded: 83\n",
      "No. Speakers loaded: 84\n",
      "No. Speakers loaded: 85\n",
      "No. Speakers loaded: 86\n",
      "No. Speakers loaded: 87\n",
      "No. Speakers loaded: 88\n",
      "No. Speakers loaded: 89\n",
      "No. Speakers loaded: 90\n",
      "No. Speakers loaded: 91\n",
      "No. Speakers loaded: 92\n",
      "No. Speakers loaded: 93\n",
      "No. Speakers loaded: 94\n",
      "No. Speakers loaded: 95\n",
      "No. Speakers loaded: 96\n",
      "No. Speakers loaded: 97\n",
      "No. Speakers loaded: 98\n",
      "No. Speakers loaded: 99\n",
      "No. Speakers loaded: 100\n",
      "No. Speakers loaded: 101\n",
      "No. Speakers loaded: 102\n",
      "No. Speakers loaded: 103\n",
      "No. Speakers loaded: 104\n",
      "No. Speakers loaded: 105\n",
      "No. Speakers loaded: 106\n",
      "No. Speakers loaded: 107\n",
      "No. Speakers loaded: 108\n",
      "No. Speakers loaded: 109\n",
      "No. Speakers loaded: 110\n"
     ]
    }
   ],
   "source": [
    "x_enroll, y_enroll = create_enrollment_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enroll = np.reshape(x_enroll, [-1,512,299,1])\n",
    "y_enroll = np.reshape(y_enroll, [-1,1])\n",
    "features_enroll = feature_model.predict(x_enroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/techresearch/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(probability=True, kernel='rbf', gamma='scale')\n",
    "svc.fit(features_enroll, y_enroll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval, y_eval = create_evaluation_dataset()\n",
    "x_eval = np.reshape(x_eval, [-1,512,299,1])\n",
    "y_eval = np.reshape(y_eval, [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_eval = feature_model.predict(x_eval)\n",
    "predictions_eval = svc.predict(features_eval)\n",
    "predictions_eval = np.reshape(predictions_eval, (-1,1))\n",
    "unique, counts = np.unique((predictions_eval == y_eval), return_counts=True)\n",
    "results_eval = dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 13, True: 18}\n",
      "Accuracy: 0.5806451612903226\n",
      "Rejected: 11\n",
      "Thresh: 0.75\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.05\n",
    "result_with_thresh = []\n",
    "\n",
    "probabilities = svc.predict_proba(features_eval)\n",
    "\n",
    "for i, pred in enumerate(predictions_eval):\n",
    "    index = np.where(svc.classes_==pred)[0][0]\n",
    "    if probabilities[i][index] > thresh:\n",
    "        if pred == y_eval[i]:\n",
    "            result_with_thresh.append(True)\n",
    "        else:\n",
    "            result_with_thresh.append(False)\n",
    "    else:\n",
    "        result_with_thresh.append('Rejected')\n",
    "        \n",
    "f = result_with_thresh.count(False)\n",
    "t = result_with_thresh.count(True)\n",
    "r = result_with_thresh.count('Rejected')\n",
    "\n",
    "print(results_eval)\n",
    "accuracy_eval = float(results_eval[True])/len(predictions_eval)\n",
    "print('Accuracy: ' + str(accuracy_eval))\n",
    "\n",
    "acc = t/(t+f)\n",
    "print('Rejected: ' + str(r))\n",
    "print('Thresh: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bob3",
   "language": "python",
   "name": "bob3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
